apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  annotations:
    argocd.argoproj.io/tracking-id: cluster-infra:argoproj.io/Application:argocd/kube-prometheus-stack
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"argoproj.io/v1alpha1","kind":"Application","metadata":{"annotations":{"argocd.argoproj.io/tracking-id":"cluster-infra:argoproj.io/Application:argocd/kube-prometheus-stack"},"finalizers":["resources-finalizer.argocd.argoproj.io"],"name":"kube-prometheus-stack","namespace":"argocd"},"spec":{"destination":{"namespace":"monitoring","server":"https://kubernetes.default.svc"},"project":"default","source":{"chart":"kube-prometheus-stack","helm":{"values":"# Ensure all lines below are correctly indented as part of this multi-line string\ngrafana:\n  # To set an initial password (will be stored in a secret):\n  adminPassword: \"resetPassword\"\n\n  # Disable problematic init container\n  initContainers: []\n\n  # Enable Ingress for Grafana\n  ingress:\n    enabled: false # This chart's ingress.enabled, not to be confused with our separate grafana-ingress.yaml\n                  # If you want ONLY your grafana-ingress.yaml to create the ingress,\n                  # you might set this to false and remove ingressClassName and pathType here.\n                  # However, setting ingressClassName here is also fine if your separate ingress is more specific\n                  # or you decide to let the chart create it and just annotate.\n                  # For full control with your separate grafana-ingress.yaml, often people set this to false.\n                  # Let's assume for now it's okay to be true, and your separate ingress takes precedence or you adapt.\n    ingressClassName: \"nginx\" # Your IngressClass name\n    pathType: Prefix\n\n  persistence:\n    enabled: true\n    # storageClassName: \"local-path\" # Or your preferred StorageClass. If null, uses default.\n    size: 10Gi\n  \n  # ADD GRAFANA RESOURCE REQUESTS - Currently using 344Mi memory, 78m CPU\n  resources:\n    requests:\n      cpu: \"100m\"        # Current: 78m, so 100m gives headroom\n      memory: \"400Mi\"    # Current: 344Mi, so 400Mi gives headroom\n    limits:\n      cpu: \"500m\"        # Allow for dashboard rendering spikes\n      memory: \"512Mi\"    # Allow for memory spikes\n\nprometheus:\n  prometheusSpec:\n    # externalLabels: # Optional: if you want to add external labels to your metrics\n    #   cluster: lab1830\n    retention: 15d # How long to keep metrics, default is 10d. Adjust as needed for your storage.\n    storageSpec:\n      volumeClaimTemplate:\n        spec:\n          # storageClassName: \"local-path\" # Or your preferred StorageClass. If null, uses default.\n          accessModes: [\"ReadWriteOnce\"]\n          resources:\n            requests:\n              storage: 50Gi # Adjust based on your needs and available storage\n    \n    # ADD PROMETHEUS RESOURCE REQUESTS - Currently using 717Mi memory, 37m CPU\n    resources:\n      requests:\n        cpu: \"100m\"        # Current: 37m, so 100m gives headroom\n        memory: \"800Mi\"    # Current: 717Mi, so 800Mi gives headroom\n      limits:\n        cpu: \"1000m\"       # Allow spikes during heavy queries\n        memory: \"1200Mi\"   # Allow for memory spikes during ingestion\n\nalertmanager:\n  alertmanagerSpec:\n    storage:\n      volumeClaimTemplate:\n        spec:\n          # storageClassName: \"local-path\"\n          accessModes: [\"ReadWriteOnce\"]\n          resources:\n            requests:\n              storage: 10Gi\n    \n    # ADD ALERTMANAGER RESOURCE REQUESTS - Currently using 38Mi memory, 3m CPU\n    resources:\n      requests:\n        cpu: \"25m\"         # Current: 3m, so 25m gives plenty headroom\n        memory: \"64Mi\"     # Current: 38Mi, so 64Mi gives headroom\n      limits:\n        cpu: \"100m\"\n        memory: \"128Mi\"\n\n# ADD RESOURCE REQUESTS FOR SMALLER COMPONENTS\nkubeStateMetrics:\n  resources:\n    requests:\n      cpu: \"25m\"          # Current: 3m\n      memory: \"32Mi\"      # Current: 17Mi\n    limits:\n      cpu: \"100m\"\n      memory: \"64Mi\"\n\n# Node exporter is a DaemonSet, so resource requests are important\nprometheus-node-exporter:\n  resources:\n    requests:\n      cpu: \"25m\"          # Currently: ~3m per node\n      memory: \"32Mi\"      # Currently: ~11Mi per node\n    limits:\n      cpu: \"100m\"\n      memory: \"64Mi\"\n\n# Disable components we might not need initially for a simpler setup\n# You can enable them later by removing these 'enabled: false' lines or setting to true.\n# prometheus-node-exporter:\n#   enabled: true # Usually good to keep\n# kube-state-metrics:\n#   enabled: true # Usually good to keep\n# alertmanager:\n#   enabled: false # Disable if you're not setting up alerts yet\n# grafana: # This top-level 'grafana.enabled' would disable Grafana deployment by the chart\n#   enabled: true # Keep this true if you want the chart to deploy Grafana\n# prometheusOperator:\n#   admissionWebhooks:\n#     patch:\n#       enabled: false # Can sometimes cause issues on some K3s setups if cert-manager webhook is also very busy, but usually fine. Let's keep default for now.\n"},"repoURL":"https://prometheus-community.github.io/helm-charts","targetRevision":"72.9.0"},"syncPolicy":{"automated":{"prune":true,"selfHeal":true},"syncOptions":["CreateNamespace=true","ServerSideApply=true"]}}}
  creationTimestamp: "2025-07-12T03:58:32Z"
  finalizers:
  - resources-finalizer.argocd.argoproj.io
  generation: 32
  name: kube-prometheus-stack
  namespace: argocd
  resourceVersion: "98462"
  uid: 04d8cc11-076d-4244-b09f-4e47a1877747
operation:
  initiatedBy:
    automated: true
  retry:
    limit: 5
  sync:
    prune: true
    revision: 72.9.0
    syncOptions:
    - CreateNamespace=true
    - ServerSideApply=true
spec:
  destination:
    namespace: monitoring
    server: https://kubernetes.default.svc
  project: default
  source:
    chart: kube-prometheus-stack
    helm:
      values: "# Ensure all lines below are correctly indented as part of this multi-line
        string\ngrafana:\n  # To set an initial password (will be stored in a secret):\n
        \ adminPassword: \"resetPassword\"\n\n  # Disable problematic init container\n
        \ initContainers: []\n\n  # Enable Ingress for Grafana\n  ingress:\n    enabled:
        false # This chart's ingress.enabled, not to be confused with our separate
        grafana-ingress.yaml\n                  # If you want ONLY your grafana-ingress.yaml
        to create the ingress,\n                  # you might set this to false and
        remove ingressClassName and pathType here.\n                  # However, setting
        ingressClassName here is also fine if your separate ingress is more specific\n
        \                 # or you decide to let the chart create it and just annotate.\n
        \                 # For full control with your separate grafana-ingress.yaml,
        often people set this to false.\n                  # Let's assume for now
        it's okay to be true, and your separate ingress takes precedence or you adapt.\n
        \   ingressClassName: \"nginx\" # Your IngressClass name\n    pathType: Prefix\n\n
        \ persistence:\n    enabled: true\n    # storageClassName: \"local-path\"
        # Or your preferred StorageClass. If null, uses default.\n    size: 10Gi\n
        \ \n  # ADD GRAFANA RESOURCE REQUESTS - Currently using 344Mi memory, 78m
        CPU\n  resources:\n    requests:\n      cpu: \"100m\"        # Current: 78m,
        so 100m gives headroom\n      memory: \"400Mi\"    # Current: 344Mi, so 400Mi
        gives headroom\n    limits:\n      cpu: \"500m\"        # Allow for dashboard
        rendering spikes\n      memory: \"512Mi\"    # Allow for memory spikes\n\nprometheus:\n
        \ prometheusSpec:\n    # externalLabels: # Optional: if you want to add external
        labels to your metrics\n    #   cluster: lab1830\n    retention: 15d # How
        long to keep metrics, default is 10d. Adjust as needed for your storage.\n
        \   storageSpec:\n      volumeClaimTemplate:\n        spec:\n          # storageClassName:
        \"local-path\" # Or your preferred StorageClass. If null, uses default.\n
        \         accessModes: [\"ReadWriteOnce\"]\n          resources:\n            requests:\n
        \             storage: 50Gi # Adjust based on your needs and available storage\n
        \   \n    # ADD PROMETHEUS RESOURCE REQUESTS - Currently using 717Mi memory,
        37m CPU\n    resources:\n      requests:\n        cpu: \"100m\"        # Current:
        37m, so 100m gives headroom\n        memory: \"800Mi\"    # Current: 717Mi,
        so 800Mi gives headroom\n      limits:\n        cpu: \"1000m\"       # Allow
        spikes during heavy queries\n        memory: \"1200Mi\"   # Allow for memory
        spikes during ingestion\n\nalertmanager:\n  alertmanagerSpec:\n    storage:\n
        \     volumeClaimTemplate:\n        spec:\n          # storageClassName: \"local-path\"\n
        \         accessModes: [\"ReadWriteOnce\"]\n          resources:\n            requests:\n
        \             storage: 10Gi\n    \n    # ADD ALERTMANAGER RESOURCE REQUESTS
        - Currently using 38Mi memory, 3m CPU\n    resources:\n      requests:\n        cpu:
        \"25m\"         # Current: 3m, so 25m gives plenty headroom\n        memory:
        \"64Mi\"     # Current: 38Mi, so 64Mi gives headroom\n      limits:\n        cpu:
        \"100m\"\n        memory: \"128Mi\"\n\n# ADD RESOURCE REQUESTS FOR SMALLER
        COMPONENTS\nkubeStateMetrics:\n  resources:\n    requests:\n      cpu: \"25m\"
        \         # Current: 3m\n      memory: \"32Mi\"      # Current: 17Mi\n    limits:\n
        \     cpu: \"100m\"\n      memory: \"64Mi\"\n\n# Node exporter is a DaemonSet,
        so resource requests are important\nprometheus-node-exporter:\n  resources:\n
        \   requests:\n      cpu: \"25m\"          # Currently: ~3m per node\n      memory:
        \"32Mi\"      # Currently: ~11Mi per node\n    limits:\n      cpu: \"100m\"\n
        \     memory: \"64Mi\"\n\n# Disable components we might not need initially
        for a simpler setup\n# You can enable them later by removing these 'enabled:
        false' lines or setting to true.\n# prometheus-node-exporter:\n#   enabled:
        true # Usually good to keep\n# kube-state-metrics:\n#   enabled: true # Usually
        good to keep\n# alertmanager:\n#   enabled: false # Disable if you're not
        setting up alerts yet\n# grafana: # This top-level 'grafana.enabled' would
        disable Grafana deployment by the chart\n#   enabled: true # Keep this true
        if you want the chart to deploy Grafana\n# prometheusOperator:\n#   admissionWebhooks:\n#
        \    patch:\n#       enabled: false # Can sometimes cause issues on some K3s
        setups if cert-manager webhook is also very busy, but usually fine. Let's
        keep default for now.\n"
    repoURL: https://prometheus-community.github.io/helm-charts
    targetRevision: 72.9.0
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    - ServerSideApply=true
status:
  controllerNamespace: argocd
  health:
    lastTransitionTime: "2025-07-12T03:59:05Z"
    status: Healthy
  operationState:
    message: waiting for completion of hook batch/Job/kube-prometheus-stack-admission-create
    operation:
      initiatedBy:
        automated: true
      retry:
        limit: 5
      sync:
        prune: true
        revision: 72.9.0
        syncOptions:
        - CreateNamespace=true
        - ServerSideApply=true
    phase: Running
    retryCount: 2
    startedAt: "2025-07-12T03:58:33Z"
    syncResult:
      resources:
      - group: ""
        hookPhase: Succeeded
        hookType: PreSync
        kind: ServiceAccount
        message: kube-prometheus-stack-admission created
        name: kube-prometheus-stack-admission
        namespace: monitoring
        syncPhase: PreSync
        version: v1
      - group: rbac.authorization.k8s.io
        hookPhase: Succeeded
        hookType: PreSync
        kind: ClusterRole
        message: kube-prometheus-stack-admission created
        name: kube-prometheus-stack-admission
        namespace: monitoring
        syncPhase: PreSync
        version: v1
      - group: rbac.authorization.k8s.io
        hookPhase: Succeeded
        hookType: PreSync
        kind: ClusterRoleBinding
        message: kube-prometheus-stack-admission created
        name: kube-prometheus-stack-admission
        namespace: monitoring
        syncPhase: PreSync
        version: v1
      - group: rbac.authorization.k8s.io
        hookPhase: Succeeded
        hookType: PreSync
        kind: Role
        message: kube-prometheus-stack-admission created
        name: kube-prometheus-stack-admission
        namespace: monitoring
        syncPhase: PreSync
        version: v1
      - group: rbac.authorization.k8s.io
        hookPhase: Succeeded
        hookType: PreSync
        kind: RoleBinding
        message: kube-prometheus-stack-admission created
        name: kube-prometheus-stack-admission
        namespace: monitoring
        syncPhase: PreSync
        version: v1
      - group: batch
        hookPhase: Running
        hookType: PreSync
        kind: Job
        message: job.batch/kube-prometheus-stack-admission-create serverside-applied
        name: kube-prometheus-stack-admission-create
        namespace: monitoring
        syncPhase: PreSync
        version: v1
      revision: 72.9.0
      source:
        chart: kube-prometheus-stack
        helm:
          values: "# Ensure all lines below are correctly indented as part of this
            multi-line string\ngrafana:\n  # To set an initial password (will be stored
            in a secret):\n  adminPassword: \"resetPassword\"\n\n  # Disable problematic
            init container\n  initContainers: []\n\n  # Enable Ingress for Grafana\n
            \ ingress:\n    enabled: false # This chart's ingress.enabled, not to
            be confused with our separate grafana-ingress.yaml\n                  #
            If you want ONLY your grafana-ingress.yaml to create the ingress,\n                  #
            you might set this to false and remove ingressClassName and pathType here.\n
            \                 # However, setting ingressClassName here is also fine
            if your separate ingress is more specific\n                  # or you
            decide to let the chart create it and just annotate.\n                  #
            For full control with your separate grafana-ingress.yaml, often people
            set this to false.\n                  # Let's assume for now it's okay
            to be true, and your separate ingress takes precedence or you adapt.\n
            \   ingressClassName: \"nginx\" # Your IngressClass name\n    pathType:
            Prefix\n\n  persistence:\n    enabled: true\n    # storageClassName: \"local-path\"
            # Or your preferred StorageClass. If null, uses default.\n    size: 10Gi\n
            \ \n  # ADD GRAFANA RESOURCE REQUESTS - Currently using 344Mi memory,
            78m CPU\n  resources:\n    requests:\n      cpu: \"100m\"        # Current:
            78m, so 100m gives headroom\n      memory: \"400Mi\"    # Current: 344Mi,
            so 400Mi gives headroom\n    limits:\n      cpu: \"500m\"        # Allow
            for dashboard rendering spikes\n      memory: \"512Mi\"    # Allow for
            memory spikes\n\nprometheus:\n  prometheusSpec:\n    # externalLabels:
            # Optional: if you want to add external labels to your metrics\n    #
            \  cluster: lab1830\n    retention: 15d # How long to keep metrics, default
            is 10d. Adjust as needed for your storage.\n    storageSpec:\n      volumeClaimTemplate:\n
            \       spec:\n          # storageClassName: \"local-path\" # Or your
            preferred StorageClass. If null, uses default.\n          accessModes:
            [\"ReadWriteOnce\"]\n          resources:\n            requests:\n              storage:
            50Gi # Adjust based on your needs and available storage\n    \n    # ADD
            PROMETHEUS RESOURCE REQUESTS - Currently using 717Mi memory, 37m CPU\n
            \   resources:\n      requests:\n        cpu: \"100m\"        # Current:
            37m, so 100m gives headroom\n        memory: \"800Mi\"    # Current: 717Mi,
            so 800Mi gives headroom\n      limits:\n        cpu: \"1000m\"       #
            Allow spikes during heavy queries\n        memory: \"1200Mi\"   # Allow
            for memory spikes during ingestion\n\nalertmanager:\n  alertmanagerSpec:\n
            \   storage:\n      volumeClaimTemplate:\n        spec:\n          # storageClassName:
            \"local-path\"\n          accessModes: [\"ReadWriteOnce\"]\n          resources:\n
            \           requests:\n              storage: 10Gi\n    \n    # ADD ALERTMANAGER
            RESOURCE REQUESTS - Currently using 38Mi memory, 3m CPU\n    resources:\n
            \     requests:\n        cpu: \"25m\"         # Current: 3m, so 25m gives
            plenty headroom\n        memory: \"64Mi\"     # Current: 38Mi, so 64Mi
            gives headroom\n      limits:\n        cpu: \"100m\"\n        memory:
            \"128Mi\"\n\n# ADD RESOURCE REQUESTS FOR SMALLER COMPONENTS\nkubeStateMetrics:\n
            \ resources:\n    requests:\n      cpu: \"25m\"          # Current: 3m\n
            \     memory: \"32Mi\"      # Current: 17Mi\n    limits:\n      cpu: \"100m\"\n
            \     memory: \"64Mi\"\n\n# Node exporter is a DaemonSet, so resource
            requests are important\nprometheus-node-exporter:\n  resources:\n    requests:\n
            \     cpu: \"25m\"          # Currently: ~3m per node\n      memory: \"32Mi\"
            \     # Currently: ~11Mi per node\n    limits:\n      cpu: \"100m\"\n
            \     memory: \"64Mi\"\n\n# Disable components we might not need initially
            for a simpler setup\n# You can enable them later by removing these 'enabled:
            false' lines or setting to true.\n# prometheus-node-exporter:\n#   enabled:
            true # Usually good to keep\n# kube-state-metrics:\n#   enabled: true
            # Usually good to keep\n# alertmanager:\n#   enabled: false # Disable
            if you're not setting up alerts yet\n# grafana: # This top-level 'grafana.enabled'
            would disable Grafana deployment by the chart\n#   enabled: true # Keep
            this true if you want the chart to deploy Grafana\n# prometheusOperator:\n#
            \  admissionWebhooks:\n#     patch:\n#       enabled: false # Can sometimes
            cause issues on some K3s setups if cert-manager webhook is also very busy,
            but usually fine. Let's keep default for now.\n"
        repoURL: https://prometheus-community.github.io/helm-charts
        targetRevision: 72.9.0
  reconciledAt: "2025-07-12T03:59:38Z"
  resourceHealthSource: appTree
  resources:
  - kind: ConfigMap
    name: kube-prometheus-stack-alertmanager-overview
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-apiserver
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-cluster-total
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-controller-manager
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-etcd
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-grafana-config-dashboards
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-grafana-datasource
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-grafana-overview
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-k8s-coredns
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-k8s-resources-cluster
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-k8s-resources-multicluster
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-k8s-resources-namespace
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-k8s-resources-node
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-k8s-resources-pod
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-k8s-resources-workload
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-k8s-resources-workloads-namespace
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-kubelet
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-namespace-by-pod
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-namespace-by-workload
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-node-cluster-rsrc-use
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-node-rsrc-use
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-nodes
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-nodes-aix
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-nodes-darwin
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-persistentvolumesusage
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-pod-total
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-prometheus
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-proxy
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-scheduler
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ConfigMap
    name: kube-prometheus-stack-workload-total
    namespace: monitoring
    status: Synced
    version: v1
  - kind: PersistentVolumeClaim
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    status: Synced
    version: v1
  - kind: Secret
    name: alertmanager-kube-prometheus-stack-alertmanager
    namespace: monitoring
    status: Synced
    version: v1
  - kind: Secret
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-coredns
    namespace: kube-system
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-kube-controller-manager
    namespace: kube-system
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-kube-etcd
    namespace: kube-system
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-kube-proxy
    namespace: kube-system
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-kube-scheduler
    namespace: kube-system
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-alertmanager
    namespace: monitoring
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-kube-state-metrics
    namespace: monitoring
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-operator
    namespace: monitoring
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-prometheus
    namespace: monitoring
    status: Synced
    version: v1
  - kind: Service
    name: kube-prometheus-stack-prometheus-node-exporter
    namespace: monitoring
    status: Synced
    version: v1
  - hook: true
    kind: ServiceAccount
    name: kube-prometheus-stack-admission
    namespace: monitoring
    requiresPruning: true
    version: v1
  - kind: ServiceAccount
    name: kube-prometheus-stack-alertmanager
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ServiceAccount
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ServiceAccount
    name: kube-prometheus-stack-kube-state-metrics
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ServiceAccount
    name: kube-prometheus-stack-operator
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ServiceAccount
    name: kube-prometheus-stack-prometheus
    namespace: monitoring
    status: Synced
    version: v1
  - kind: ServiceAccount
    name: kube-prometheus-stack-prometheus-node-exporter
    namespace: monitoring
    status: Synced
    version: v1
  - group: admissionregistration.k8s.io
    kind: MutatingWebhookConfiguration
    name: kube-prometheus-stack-admission
    status: Synced
    version: v1
  - group: admissionregistration.k8s.io
    kind: ValidatingWebhookConfiguration
    name: kube-prometheus-stack-admission
    status: Synced
    version: v1
  - group: apiextensions.k8s.io
    kind: CustomResourceDefinition
    name: alertmanagerconfigs.monitoring.coreos.com
    status: Synced
    version: v1
  - group: apiextensions.k8s.io
    kind: CustomResourceDefinition
    name: alertmanagers.monitoring.coreos.com
    status: Synced
    version: v1
  - group: apiextensions.k8s.io
    kind: CustomResourceDefinition
    name: podmonitors.monitoring.coreos.com
    status: Synced
    version: v1
  - group: apiextensions.k8s.io
    kind: CustomResourceDefinition
    name: probes.monitoring.coreos.com
    status: Synced
    version: v1
  - group: apiextensions.k8s.io
    kind: CustomResourceDefinition
    name: prometheusagents.monitoring.coreos.com
    status: Synced
    version: v1
  - group: apiextensions.k8s.io
    kind: CustomResourceDefinition
    name: prometheuses.monitoring.coreos.com
    status: Synced
    version: v1
  - group: apiextensions.k8s.io
    kind: CustomResourceDefinition
    name: prometheusrules.monitoring.coreos.com
    status: Synced
    version: v1
  - group: apiextensions.k8s.io
    kind: CustomResourceDefinition
    name: scrapeconfigs.monitoring.coreos.com
    status: Synced
    version: v1
  - group: apiextensions.k8s.io
    kind: CustomResourceDefinition
    name: servicemonitors.monitoring.coreos.com
    status: Synced
    version: v1
  - group: apiextensions.k8s.io
    kind: CustomResourceDefinition
    name: thanosrulers.monitoring.coreos.com
    status: Synced
    version: v1
  - group: apps
    kind: DaemonSet
    name: kube-prometheus-stack-prometheus-node-exporter
    namespace: monitoring
    status: Synced
    version: v1
  - group: apps
    kind: Deployment
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    status: Synced
    version: v1
  - group: apps
    kind: Deployment
    name: kube-prometheus-stack-kube-state-metrics
    namespace: monitoring
    status: Synced
    version: v1
  - group: apps
    kind: Deployment
    name: kube-prometheus-stack-operator
    namespace: monitoring
    status: Synced
    version: v1
  - group: batch
    hook: true
    kind: Job
    name: kube-prometheus-stack-admission-create
    namespace: monitoring
    requiresPruning: true
    version: v1
  - group: monitoring.coreos.com
    kind: Alertmanager
    name: kube-prometheus-stack-alertmanager
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: Prometheus
    name: kube-prometheus-stack-prometheus
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-alertmanager.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-config-reloaders
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-etcd
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-general.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-k8s.rules.container-cpu-usage-seconds-tot
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-k8s.rules.container-memory-cache
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-k8s.rules.container-memory-rss
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-k8s.rules.container-memory-swap
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-k8s.rules.container-memory-working-set-by
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-k8s.rules.container-resource
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-k8s.rules.pod-owner
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kube-apiserver-availability.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kube-apiserver-burnrate.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kube-apiserver-histogram.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kube-apiserver-slos
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kube-prometheus-general.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kube-prometheus-node-recording.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kube-scheduler.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kube-state-metrics
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kubelet.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kubernetes-apps
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kubernetes-resources
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kubernetes-storage
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kubernetes-system
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kubernetes-system-apiserver
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kubernetes-system-controller-manager
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kubernetes-system-kube-proxy
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kubernetes-system-kubelet
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-kubernetes-system-scheduler
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-node-exporter
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-node-exporter.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-node-network
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-node.rules
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-prometheus
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: PrometheusRule
    name: kube-prometheus-stack-prometheus-operator
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-alertmanager
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-apiserver
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-coredns
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-kube-controller-manager
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-kube-etcd
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-kube-proxy
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-kube-scheduler
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-kube-state-metrics
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-kubelet
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-operator
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-prometheus
    namespace: monitoring
    status: Synced
    version: v1
  - group: monitoring.coreos.com
    kind: ServiceMonitor
    name: kube-prometheus-stack-prometheus-node-exporter
    namespace: monitoring
    status: Synced
    version: v1
  - group: rbac.authorization.k8s.io
    hook: true
    kind: ClusterRole
    name: kube-prometheus-stack-admission
    requiresPruning: true
    version: v1
  - group: rbac.authorization.k8s.io
    kind: ClusterRole
    name: kube-prometheus-stack-grafana-clusterrole
    status: Synced
    version: v1
  - group: rbac.authorization.k8s.io
    kind: ClusterRole
    name: kube-prometheus-stack-kube-state-metrics
    status: Synced
    version: v1
  - group: rbac.authorization.k8s.io
    kind: ClusterRole
    name: kube-prometheus-stack-operator
    status: Synced
    version: v1
  - group: rbac.authorization.k8s.io
    kind: ClusterRole
    name: kube-prometheus-stack-prometheus
    status: Synced
    version: v1
  - group: rbac.authorization.k8s.io
    hook: true
    kind: ClusterRoleBinding
    name: kube-prometheus-stack-admission
    requiresPruning: true
    version: v1
  - group: rbac.authorization.k8s.io
    kind: ClusterRoleBinding
    name: kube-prometheus-stack-grafana-clusterrolebinding
    status: Synced
    version: v1
  - group: rbac.authorization.k8s.io
    kind: ClusterRoleBinding
    name: kube-prometheus-stack-kube-state-metrics
    status: Synced
    version: v1
  - group: rbac.authorization.k8s.io
    kind: ClusterRoleBinding
    name: kube-prometheus-stack-operator
    status: Synced
    version: v1
  - group: rbac.authorization.k8s.io
    kind: ClusterRoleBinding
    name: kube-prometheus-stack-prometheus
    status: Synced
    version: v1
  - group: rbac.authorization.k8s.io
    hook: true
    kind: Role
    name: kube-prometheus-stack-admission
    namespace: monitoring
    requiresPruning: true
    version: v1
  - group: rbac.authorization.k8s.io
    kind: Role
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    status: Synced
    version: v1
  - group: rbac.authorization.k8s.io
    hook: true
    kind: RoleBinding
    name: kube-prometheus-stack-admission
    namespace: monitoring
    requiresPruning: true
    version: v1
  - group: rbac.authorization.k8s.io
    kind: RoleBinding
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    status: Synced
    version: v1
  sourceHydrator: {}
  sourceType: Helm
  summary:
    images:
    - docker.io/grafana/grafana:12.0.0-security-01
    - docker.io/library/busybox:1.31.1
    - quay.io/kiwigrid/k8s-sidecar:1.30.0
    - quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
    - quay.io/prometheus-operator/prometheus-operator:v0.82.2
    - quay.io/prometheus/alertmanager:v0.28.1
    - quay.io/prometheus/node-exporter:v1.9.1
    - quay.io/prometheus/prometheus:v3.4.1
    - registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3
    - registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
  sync:
    comparedTo:
      destination:
        namespace: monitoring
        server: https://kubernetes.default.svc
      source:
        chart: kube-prometheus-stack
        helm:
          values: "# Ensure all lines below are correctly indented as part of this
            multi-line string\ngrafana:\n  # To set an initial password (will be stored
            in a secret):\n  adminPassword: \"resetPassword\"\n\n  # Disable problematic
            init container\n  initContainers: []\n\n  # Enable Ingress for Grafana\n
            \ ingress:\n    enabled: false # This chart's ingress.enabled, not to
            be confused with our separate grafana-ingress.yaml\n                  #
            If you want ONLY your grafana-ingress.yaml to create the ingress,\n                  #
            you might set this to false and remove ingressClassName and pathType here.\n
            \                 # However, setting ingressClassName here is also fine
            if your separate ingress is more specific\n                  # or you
            decide to let the chart create it and just annotate.\n                  #
            For full control with your separate grafana-ingress.yaml, often people
            set this to false.\n                  # Let's assume for now it's okay
            to be true, and your separate ingress takes precedence or you adapt.\n
            \   ingressClassName: \"nginx\" # Your IngressClass name\n    pathType:
            Prefix\n\n  persistence:\n    enabled: true\n    # storageClassName: \"local-path\"
            # Or your preferred StorageClass. If null, uses default.\n    size: 10Gi\n
            \ \n  # ADD GRAFANA RESOURCE REQUESTS - Currently using 344Mi memory,
            78m CPU\n  resources:\n    requests:\n      cpu: \"100m\"        # Current:
            78m, so 100m gives headroom\n      memory: \"400Mi\"    # Current: 344Mi,
            so 400Mi gives headroom\n    limits:\n      cpu: \"500m\"        # Allow
            for dashboard rendering spikes\n      memory: \"512Mi\"    # Allow for
            memory spikes\n\nprometheus:\n  prometheusSpec:\n    # externalLabels:
            # Optional: if you want to add external labels to your metrics\n    #
            \  cluster: lab1830\n    retention: 15d # How long to keep metrics, default
            is 10d. Adjust as needed for your storage.\n    storageSpec:\n      volumeClaimTemplate:\n
            \       spec:\n          # storageClassName: \"local-path\" # Or your
            preferred StorageClass. If null, uses default.\n          accessModes:
            [\"ReadWriteOnce\"]\n          resources:\n            requests:\n              storage:
            50Gi # Adjust based on your needs and available storage\n    \n    # ADD
            PROMETHEUS RESOURCE REQUESTS - Currently using 717Mi memory, 37m CPU\n
            \   resources:\n      requests:\n        cpu: \"100m\"        # Current:
            37m, so 100m gives headroom\n        memory: \"800Mi\"    # Current: 717Mi,
            so 800Mi gives headroom\n      limits:\n        cpu: \"1000m\"       #
            Allow spikes during heavy queries\n        memory: \"1200Mi\"   # Allow
            for memory spikes during ingestion\n\nalertmanager:\n  alertmanagerSpec:\n
            \   storage:\n      volumeClaimTemplate:\n        spec:\n          # storageClassName:
            \"local-path\"\n          accessModes: [\"ReadWriteOnce\"]\n          resources:\n
            \           requests:\n              storage: 10Gi\n    \n    # ADD ALERTMANAGER
            RESOURCE REQUESTS - Currently using 38Mi memory, 3m CPU\n    resources:\n
            \     requests:\n        cpu: \"25m\"         # Current: 3m, so 25m gives
            plenty headroom\n        memory: \"64Mi\"     # Current: 38Mi, so 64Mi
            gives headroom\n      limits:\n        cpu: \"100m\"\n        memory:
            \"128Mi\"\n\n# ADD RESOURCE REQUESTS FOR SMALLER COMPONENTS\nkubeStateMetrics:\n
            \ resources:\n    requests:\n      cpu: \"25m\"          # Current: 3m\n
            \     memory: \"32Mi\"      # Current: 17Mi\n    limits:\n      cpu: \"100m\"\n
            \     memory: \"64Mi\"\n\n# Node exporter is a DaemonSet, so resource
            requests are important\nprometheus-node-exporter:\n  resources:\n    requests:\n
            \     cpu: \"25m\"          # Currently: ~3m per node\n      memory: \"32Mi\"
            \     # Currently: ~11Mi per node\n    limits:\n      cpu: \"100m\"\n
            \     memory: \"64Mi\"\n\n# Disable components we might not need initially
            for a simpler setup\n# You can enable them later by removing these 'enabled:
            false' lines or setting to true.\n# prometheus-node-exporter:\n#   enabled:
            true # Usually good to keep\n# kube-state-metrics:\n#   enabled: true
            # Usually good to keep\n# alertmanager:\n#   enabled: false # Disable
            if you're not setting up alerts yet\n# grafana: # This top-level 'grafana.enabled'
            would disable Grafana deployment by the chart\n#   enabled: true # Keep
            this true if you want the chart to deploy Grafana\n# prometheusOperator:\n#
            \  admissionWebhooks:\n#     patch:\n#       enabled: false # Can sometimes
            cause issues on some K3s setups if cert-manager webhook is also very busy,
            but usually fine. Let's keep default for now.\n"
        repoURL: https://prometheus-community.github.io/helm-charts
        targetRevision: 72.9.0
    revision: 72.9.0
    status: Synced
